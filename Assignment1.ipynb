{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "When solving this task, we expect you'll face (and successfully deal with) some problems or make up the ideas of the model improvement. Some of them are: \n",
        "\n",
        "- solving a problem of n-grams frequencies storing for a large corpus;\n",
        "- taking into account keyboard layout and associated misspellings;\n",
        "- efficiency improvement to make the solution faster;\n",
        "- ...\n",
        "\n",
        "Please don't forget to describe such cases, and what you decided to do with them, in the Justification section.\n",
        "\n",
        "##### IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoQeEsZvHvvi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model...\n",
            "Downloading corpus from https://norvig.com/big.txt...\n",
            "Download complete.\n",
            "Loading pre-trained model from spelling_model_corpus.txt.pkl...\n",
            "Model loaded. Vocabulary size: 32198\n",
            "\n",
            "Testing on example sentences:\n",
            "Original: I dking sport every day\n",
            "Corrected: I king sport every day\n",
            "\n",
            "Original: The dking species is becoming rare\n",
            "Corrected: the king species is becoming rare\n",
            "\n",
            "Original: She has a bewtiful voice\n",
            "Corrected: she has a beautiful voice\n",
            "\n",
            "Original: The waether is nice today\n",
            "Corrected: the weather is nice today\n",
            "\n",
            "Original: I cant beleive I won the competiton\n",
            "Corrected: I can believe I won the competition\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "import collections\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "import os\n",
        "import pickle\n",
        "from collections import defaultdict, Counter\n",
        "from typing import List, Dict, Tuple, Set, Optional\n",
        "import time\n",
        "import random\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "import heapq\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "class ContextSpellingCorrector:\n",
        "    def __init__(self, corpus_path=None, n_gram_max=3, use_keyboard_distance=True):\n",
        "        self.n_gram_max = n_gram_max\n",
        "        self.use_keyboard_distance = use_keyboard_distance\n",
        "        \n",
        "        self.word_counts = Counter()\n",
        "        self.n_grams = {n: defaultdict(int) for n in range(2, n_gram_max + 1)}\n",
        "        self.total_words = 0\n",
        "        \n",
        "        self.keyboard_neighbors = self._build_keyboard_layout()\n",
        "        \n",
        "        if corpus_path:\n",
        "            self.train(corpus_path)\n",
        "    \n",
        "    def _build_keyboard_layout(self) -> Dict[str, List[str]]:\n",
        "        \"\"\"Build a mapping of keyboard layout neighbors.\"\"\"\n",
        "        qwerty_layout = [\n",
        "            \"qwertyuiop\",\n",
        "            \"asdfghjkl\",\n",
        "            \"zxcvbnm\"\n",
        "        ]\n",
        "        \n",
        "        neighbors = defaultdict(list)\n",
        "        \n",
        "        for row_idx, row in enumerate(qwerty_layout):\n",
        "            for col_idx, char in enumerate(row):\n",
        "                if col_idx > 0:\n",
        "                    neighbors[char].append(row[col_idx - 1])\n",
        "                if col_idx < len(row) - 1:\n",
        "                    neighbors[char].append(row[col_idx + 1])\n",
        "                \n",
        "                if row_idx > 0 and col_idx < len(qwerty_layout[row_idx - 1]):\n",
        "                    neighbors[char].append(qwerty_layout[row_idx - 1][col_idx])\n",
        "                if row_idx < len(qwerty_layout) - 1 and col_idx < len(qwerty_layout[row_idx + 1]):\n",
        "                    neighbors[char].append(qwerty_layout[row_idx + 1][col_idx])\n",
        "        \n",
        "        return neighbors\n",
        "    \n",
        "    def train(self, corpus_path: str, save_model=True) -> None:\n",
        "        print(\"Training model...\")\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Download corpus if it's a URL\n",
        "        if corpus_path.startswith('http'):\n",
        "            print(f\"Downloading corpus from {corpus_path}...\")\n",
        "            local_file = \"corpus.txt\"\n",
        "            urllib.request.urlretrieve(corpus_path, local_file)\n",
        "            corpus_path = local_file\n",
        "            print(\"Download complete.\")\n",
        "        \n",
        "        model_file = f\"spelling_model_{os.path.basename(corpus_path)}.pkl\"\n",
        "        if os.path.exists(model_file):\n",
        "            print(f\"Loading pre-trained model from {model_file}...\")\n",
        "            with open(model_file, 'rb') as f:\n",
        "                model_data = pickle.load(f)\n",
        "                self.word_counts = model_data['word_counts']\n",
        "                self.n_grams = model_data['n_grams']\n",
        "                self.total_words = model_data['total_words']\n",
        "            print(f\"Model loaded. Vocabulary size: {len(self.word_counts)}\")\n",
        "            return\n",
        "        \n",
        "        with open(corpus_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "            text = file.read().lower()\n",
        "        \n",
        "        words = re.findall(r'\\w+', text)\n",
        "        self.total_words = len(words)\n",
        "        \n",
        "        self.word_counts = Counter(words)\n",
        "        \n",
        "        for n in range(2, self.n_gram_max + 1):\n",
        "            print(f\"Generating {n}-grams...\")\n",
        "            n_gram_list = list(ngrams(words, n))\n",
        "            for gram in n_gram_list:\n",
        "                self.n_grams[n][gram] += 1\n",
        "        \n",
        "        if save_model:\n",
        "            with open(model_file, 'wb') as f:\n",
        "                pickle.dump({\n",
        "                    'word_counts': self.word_counts,\n",
        "                    'n_grams': self.n_grams,\n",
        "                    'total_words': self.total_words\n",
        "                }, f)\n",
        "            print(f\"Model saved to {model_file}\")\n",
        "        \n",
        "        print(f\"Training completed in {time.time() - start_time:.2f} seconds\")\n",
        "        print(f\"Vocabulary size: {len(self.word_counts)}\")\n",
        "    \n",
        "    def _edits1(self, word: str) -> Set[str]:\n",
        "        letters = string.ascii_lowercase\n",
        "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "        \n",
        "        deletes = [L + R[1:] for L, R in splits if R]\n",
        "        \n",
        "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
        "        \n",
        "        if self.use_keyboard_distance:\n",
        "            replaces = []\n",
        "            for L, R in splits:\n",
        "                if R:\n",
        "                    char = R[0]\n",
        "                    if char in self.keyboard_neighbors:\n",
        "                        for neighbor in self.keyboard_neighbors[char]:\n",
        "                            replaces.append(L + neighbor + R[1:])\n",
        "                    for c in letters:\n",
        "                        if c != char:\n",
        "                            replaces.append(L + c + R[1:])\n",
        "        else:\n",
        "            replaces = [L + c + R[1:] for L, R in splits if R for c in letters if c != R[0]]\n",
        "        \n",
        "        inserts = [L + c + R for L, R in splits for c in letters]\n",
        "        \n",
        "        return set(deletes + transposes + replaces + inserts)\n",
        "    \n",
        "    def _edits2(self, word: str) -> Set[str]:\n",
        "        return {e2 for e1 in self._edits1(word) for e2 in self._edits1(e1)}\n",
        "    \n",
        "    def known(self, words: Set[str]) -> List[str]:\n",
        "        return [w for w in words if w in self.word_counts]\n",
        "    \n",
        "    def word_probability(self, word: str) -> float:\n",
        "        return self.word_counts[word] / self.total_words if self.total_words > 0 else 0\n",
        "    \n",
        "    def candidates(self, word: str) -> List[str]:\n",
        "        if not word:\n",
        "            return []\n",
        "            \n",
        "        if word in self.word_counts:\n",
        "            return [word]\n",
        "            \n",
        "        edit1_candidates = self.known(self._edits1(word))\n",
        "        if edit1_candidates:\n",
        "            return edit1_candidates\n",
        "            \n",
        "        edit2_candidates = self.known(self._edits2(word))\n",
        "        if edit2_candidates:\n",
        "            return edit2_candidates\n",
        "            \n",
        "        return [word]\n",
        "    \n",
        "    def ngram_probability(self, context: Tuple[str], candidate: str, smoothing=0.1) -> float:\n",
        "        n = len(context) + 1\n",
        "        \n",
        "        if n <= 1 or n > self.n_gram_max:\n",
        "            return self.word_probability(candidate)\n",
        "        \n",
        "        ngram = context + (candidate,)\n",
        "        \n",
        "        ngram_count = self.n_grams[n].get(ngram, 0)\n",
        "        \n",
        "        context_count = 0\n",
        "        for gram in self.n_grams[n]:\n",
        "            if gram[:-1] == context:\n",
        "                context_count += self.n_grams[n][gram]\n",
        "        \n",
        "        if context_count == 0:\n",
        "            if n > 2:\n",
        "                return self.ngram_probability(context[1:], candidate, smoothing)\n",
        "            else:\n",
        "                return self.word_probability(candidate)\n",
        "        \n",
        "        # Лапласовское сглаживание\n",
        "        return (ngram_count + smoothing) / (context_count + smoothing * len(self.word_counts))\n",
        "    \n",
        "    def correct_word(self, word: str, context: List[str]=None, beam_size=5) -> str:\n",
        "        # кто-то засунул пустой словарь\n",
        "        if not word or len(word) <= 1:\n",
        "            return word\n",
        "        \n",
        "        word = word.lower()\n",
        "        \n",
        "        if word in self.word_counts and self.word_counts[word] > 2:\n",
        "            return word\n",
        "        \n",
        "        word_candidates = self.candidates(word)\n",
        "        \n",
        "        if not word_candidates:\n",
        "            return word\n",
        "        \n",
        "        if not context or len(word_candidates) == 1:\n",
        "            return max(word_candidates, key=self.word_probability)\n",
        "        \n",
        "        context_size = min(len(context), self.n_gram_max - 1)\n",
        "        context_tuple = tuple(context[-context_size:]) if context_size > 0 else ()\n",
        "        \n",
        "        candidates_with_scores = []\n",
        "        for candidate in word_candidates:\n",
        "            unigram_prob = self.word_probability(candidate)\n",
        "            \n",
        "            edit_distance = 0\n",
        "            if candidate != word:\n",
        "                edit_distance = 1 if candidate in self._edits1(word) else 2\n",
        "            \n",
        "            if context_tuple:\n",
        "                context_prob = self.ngram_probability(context_tuple, candidate)\n",
        "            else:\n",
        "                context_prob = unigram_prob\n",
        "            \n",
        "            # ВЫБРАНЫ ЭМПИРИЧЕСКИ\n",
        "            score = (0.3 * unigram_prob + 0.7 * context_prob) / (1 + 0.5 * edit_distance)\n",
        "            candidates_with_scores.append((candidate, score))\n",
        "        \n",
        "        best_candidate = max(candidates_with_scores, key=lambda x: x[1])[0]\n",
        "        \n",
        "        return best_candidate\n",
        "    \n",
        "    def correct_text(self, text: str) -> str:\n",
        "        words = []\n",
        "        non_words = []\n",
        "        current_word = \"\"\n",
        "        current_non_word = \"\"\n",
        "        \n",
        "        for char in text:\n",
        "            if char.isalnum() or char == \"'\":\n",
        "                if current_non_word:\n",
        "                    non_words.append(current_non_word)\n",
        "                    current_non_word = \"\"\n",
        "                current_word += char\n",
        "            else:\n",
        "                if current_word:\n",
        "                    words.append(current_word)\n",
        "                    current_word = \"\"\n",
        "                current_non_word += char\n",
        "        \n",
        "        if current_word:\n",
        "            words.append(current_word)\n",
        "        if current_non_word:\n",
        "            non_words.append(current_non_word)\n",
        "        \n",
        "        if len(non_words) < len(words):\n",
        "            non_words.append(\"\")\n",
        "        \n",
        "        corrected_words = []\n",
        "        context = []\n",
        "        \n",
        "        for word in words:\n",
        "            if len(word) > 1:\n",
        "                corrected = self.correct_word(word, context)\n",
        "                corrected_words.append(corrected)\n",
        "                context.append(corrected)\n",
        "                if len(context) > self.n_gram_max - 1:\n",
        "                    context = context[-(self.n_gram_max - 1):]\n",
        "            else:\n",
        "                corrected_words.append(word)\n",
        "        \n",
        "        result = \"\"\n",
        "        for i in range(len(corrected_words)):\n",
        "            result += corrected_words[i]\n",
        "            if i < len(non_words):\n",
        "                result += non_words[i]\n",
        "        \n",
        "        if len(non_words) > len(corrected_words):\n",
        "            result += non_words[len(corrected_words)]\n",
        "        \n",
        "        return result\n",
        "    \n",
        "    def correct_sentences(self, sentences: List[str]) -> List[str]:\n",
        "        return [self.correct_text(sentence) for sentence in sentences]\n",
        "    \n",
        "    def evaluate(self, test_data: List[Tuple[str, str]]) -> Dict:\n",
        "        total_words = 0\n",
        "        corrected_words = 0\n",
        "        \n",
        "        start_time = time.time()\n",
        "        \n",
        "        for misspelled, correct in test_data:\n",
        "            corrected = self.correct_text(misspelled)\n",
        "            \n",
        "            misspelled_words = re.findall(r'\\w+', misspelled.lower())\n",
        "            correct_words = re.findall(r'\\w+', correct.lower())\n",
        "            corrected_words_list = re.findall(r'\\w+', corrected.lower())\n",
        "            \n",
        "            for i, (m, c) in enumerate(zip(misspelled_words, correct_words)):\n",
        "                if i < len(corrected_words_list):\n",
        "                    total_words += 1\n",
        "                    if m != c and corrected_words_list[i] == c:\n",
        "                        corrected_words += 1\n",
        "        \n",
        "        accuracy = corrected_words / total_words if total_words > 0 else 0\n",
        "        time_taken = time.time() - start_time\n",
        "        \n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'corrected_words': corrected_words,\n",
        "            'total_words': total_words,\n",
        "            'time_taken': time_taken\n",
        "        }\n",
        "\n",
        "def main():\n",
        "    corpus_url = \"https://norvig.com/big.txt\"\n",
        "    \n",
        "    corrector = ContextSpellingCorrector(corpus_url, n_gram_max=3)\n",
        "    \n",
        "    test_sentences = [\n",
        "        \"I dking sport every day\",\n",
        "        \"The dking species is becoming rare\",\n",
        "        \"She has a bewtiful voice\",\n",
        "        \"The waether is nice today\",\n",
        "        \"I cant beleive I won the competiton\"\n",
        "    ]\n",
        "    \n",
        "    print(\"\\nTesting on example sentences:\")\n",
        "    for sentence in test_sentences:\n",
        "        corrected = corrector.correct_text(sentence)\n",
        "        print(f\"Original: {sentence}\")\n",
        "        print(f\"Corrected: {corrected}\")\n",
        "        print()\n",
        "\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Justification of Implementation Choices\n",
        "\n",
        "I chose trigrams (n=3) for my context-sensitive spelling corrector as they provide sufficient context while avoiding data sparsity issues. For efficiency, I implemented pickle-based model serialization and dictionary-based n-gram storage. The keyboard layout awareness generates more realistic edit suggestions by considering adjacent keys. My scoring function combines unigram probability (30%), context probability (70%), and an edit distance penalty, with Laplace smoothing (parameter 0.1) for unseen n-grams. When higher-order n-grams aren't available, the model backs off to lower-order n-grams. The implementation preserves all non-word tokens and minimizes corrections for known words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Norvig Spell Checker Accuracy: 100.00%\n"
          ]
        }
      ],
      "source": [
        "class NorvigSpellChecker:\n",
        "    def __init__(self, corpus_path=None):\n",
        "        self.word_counts = Counter()\n",
        "        self.total_words = 0\n",
        "        \n",
        "        if corpus_path:\n",
        "            if corpus_path.startswith('http'):\n",
        "                local_file = \"norvig_corpus.txt\"\n",
        "                urllib.request.urlretrieve(corpus_path, local_file)\n",
        "                corpus_path = local_file\n",
        "            \n",
        "            with open(corpus_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "                text = file.read().lower()\n",
        "            \n",
        "            words = re.findall(r'\\w+', text)\n",
        "            self.word_counts = Counter(words)\n",
        "            self.total_words = len(words)\n",
        "    \n",
        "    def _edits1(self, word):\n",
        "        letters = string.ascii_lowercase\n",
        "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "        deletes = [L + R[1:] for L, R in splits if R]\n",
        "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
        "        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
        "        inserts = [L + c + R for L, R in splits for c in letters]\n",
        "        return set(deletes + transposes + replaces + inserts)\n",
        "    \n",
        "    def _edits2(self, word):\n",
        "        return {e2 for e1 in self._edits1(word) for e2 in self._edits1(e1)}\n",
        "    \n",
        "    def known(self, words):\n",
        "        return [w for w in words if w in self.word_counts]\n",
        "    \n",
        "    def candidates(self, word):\n",
        "        return (self.known([word]) or \n",
        "                self.known(self._edits1(word)) or \n",
        "                self.known(self._edits2(word)) or \n",
        "                [word])\n",
        "    \n",
        "    def correct(self, word):\n",
        "        return max(self.candidates(word), key=self.word_counts.get)\n",
        "    \n",
        "    def correct_text(self, text):\n",
        "        return ' '.join(self.correct(word) for word in text.lower().split())\n",
        "    \n",
        "norvig_checker = NorvigSpellChecker('useful data/bigrams (2).txt')\n",
        "\n",
        "test_words = ['abandoned', 'abdominal', 'aberrant', 'abiding', 'abject', 'abnormal', 'aboriginal']\n",
        "\n",
        "correct_count = 0\n",
        "total_count = len(test_words)\n",
        "\n",
        "for word in test_words:\n",
        "    noisy_word = word[:len(word)//2] + word[len(word)//2+1:]\n",
        "    corrected_word = norvig_checker.correct(noisy_word)\n",
        "    if corrected_word == word:\n",
        "        correct_count += 1\n",
        "\n",
        "accuracy = correct_count / total_count\n",
        "print(f\"Norvig Spell Checker Accuracy: {accuracy:.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity (or just take another dataset). Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OwZWaX9VVs7B"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model...\n",
            "Downloading corpus from https://norvig.com/big.txt...\n",
            "Download complete.\n",
            "Loading pre-trained model from spelling_model_corpus.txt.pkl...\n",
            "Model loaded. Vocabulary size: 32198\n",
            "\n",
            "Testing on example sentences:\n",
            "Original: I dking sport every day\n",
            "Corrected: I king sport every day\n",
            "\n",
            "Original: The dking species is becoming rare\n",
            "Corrected: the king species is becoming rare\n",
            "\n",
            "Original: She has a bewtiful voice\n",
            "Corrected: she has a beautiful voice\n",
            "\n",
            "Original: The waether is nice today\n",
            "Corrected: the weather is nice today\n",
            "\n",
            "Original: I cant beleive I won the competiton\n",
            "Corrected: I can believe I won the competition\n",
            "\n",
            "\n",
            "Generating test set with noise probability 0.1...\n",
            "Evaluating correctors on test set with noise probability 0.1...\n",
            "Initializing context-sensitive corrector...\n",
            "Training model...\n",
            "Downloading corpus from https://norvig.com/big.txt...\n",
            "Download complete.\n",
            "Loading pre-trained model from spelling_model_corpus.txt.pkl...\n",
            "Model loaded. Vocabulary size: 32198\n",
            "Initializing Norvig corrector...\n",
            "Evaluating context-sensitive corrector...\n",
            "Evaluating Norvig corrector...\n",
            "\n",
            "Results for noise probability 0.1:\n",
            "Context-sensitive corrector accuracy: 0.0638\n",
            "Norvig corrector accuracy: 0.0604\n",
            "Context-sensitive time: 11.43 seconds\n",
            "Norvig time: 2.47 seconds\n",
            "\n",
            "Generating test set with noise probability 0.2...\n",
            "Evaluating correctors on test set with noise probability 0.2...\n",
            "Initializing context-sensitive corrector...\n",
            "Training model...\n",
            "Downloading corpus from https://norvig.com/big.txt...\n",
            "Download complete.\n",
            "Loading pre-trained model from spelling_model_corpus.txt.pkl...\n",
            "Model loaded. Vocabulary size: 32198\n",
            "Initializing Norvig corrector...\n",
            "Evaluating context-sensitive corrector...\n",
            "Evaluating Norvig corrector...\n",
            "\n",
            "Results for noise probability 0.2:\n",
            "Context-sensitive corrector accuracy: 0.1076\n",
            "Norvig corrector accuracy: 0.0971\n",
            "Context-sensitive time: 15.17 seconds\n",
            "Norvig time: 2.99 seconds\n",
            "\n",
            "Generating test set with noise probability 0.3...\n",
            "Evaluating correctors on test set with noise probability 0.3...\n",
            "Initializing context-sensitive corrector...\n",
            "Training model...\n",
            "Downloading corpus from https://norvig.com/big.txt...\n",
            "Download complete.\n",
            "Loading pre-trained model from spelling_model_corpus.txt.pkl...\n",
            "Model loaded. Vocabulary size: 32198\n",
            "Initializing Norvig corrector...\n",
            "Evaluating context-sensitive corrector...\n",
            "Evaluating Norvig corrector...\n",
            "\n",
            "Results for noise probability 0.3:\n",
            "Context-sensitive corrector accuracy: 0.1721\n",
            "Norvig corrector accuracy: 0.1519\n",
            "Context-sensitive time: 24.67 seconds\n",
            "Norvig time: 2.03 seconds\n",
            "\n",
            "Summary:\n",
            "Noise\tContext\tNorvig\tImprovement\n",
            "0.1\t0.0638\t0.0604\t5.60%\n",
            "0.2\t0.1076\t0.0971\t10.91%\n",
            "0.3\t0.1721\t0.1519\t13.29%\n"
          ]
        }
      ],
      "source": [
        "def generate_test_set(source_text, noise_probability=0.1, test_size=100):\n",
        "    \"\"\"\n",
        "    Generate a test set by introducing spelling errors to a source text.\n",
        "    \"\"\"\n",
        "    if isinstance(source_text, str) and os.path.exists(source_text):\n",
        "        with open(source_text, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            source_text = f.read()\n",
        "    \n",
        "    sentences = nltk.sent_tokenize(source_text)\n",
        "    \n",
        "    sentences = [s for s in sentences if 5 <= len(s.split()) <= 20]\n",
        "    \n",
        "    if len(sentences) > test_size:\n",
        "        sentences = random.sample(sentences, test_size)\n",
        "    \n",
        "    test_pairs = []\n",
        "    \n",
        "    keyboard_layout = {\n",
        "        'q': 'was', 'w': 'qase', 'e': 'wsdr', 'r': 'edft', 't': 'rfgy',\n",
        "        'y': 'tghu', 'u': 'yhji', 'i': 'ujko', 'o': 'iklp', 'p': 'ol',\n",
        "        'a': 'qwsz', 's': 'awedxz', 'd': 'serfcx', 'f': 'drtgvc', 'g': 'ftyhbv',\n",
        "        'h': 'gyujnb', 'j': 'huikmn', 'k': 'jiolm', 'l': 'kop',\n",
        "        'z': 'asx', 'x': 'zsdc', 'c': 'xdfv', 'v': 'cfgb', 'b': 'vghn',\n",
        "        'n': 'bhjm', 'm': 'njk'\n",
        "    }\n",
        "    \n",
        "    def introduce_error(word):\n",
        "        if len(word) <= 2 or random.random() > noise_probability:\n",
        "            return word\n",
        "        \n",
        "        error_type = random.choice(['insert', 'delete', 'replace', 'transpose'])\n",
        "        \n",
        "        if error_type == 'insert' and len(word) > 0:\n",
        "            pos = random.randint(0, len(word))\n",
        "            char = random.choice(string.ascii_lowercase)\n",
        "            return word[:pos] + char + word[pos:]\n",
        "        \n",
        "        elif error_type == 'delete' and len(word) > 1:\n",
        "            pos = random.randint(0, len(word) - 1)\n",
        "            return word[:pos] + word[pos+1:]\n",
        "        \n",
        "        elif error_type == 'replace' and len(word) > 0:\n",
        "            pos = random.randint(0, len(word) - 1)\n",
        "            if word[pos] in keyboard_layout:\n",
        "                char = random.choice(keyboard_layout[word[pos]])\n",
        "            else:\n",
        "                char = random.choice(string.ascii_lowercase)\n",
        "            return word[:pos] + char + word[pos+1:]\n",
        "        \n",
        "        elif error_type == 'transpose' and len(word) > 1:\n",
        "            pos = random.randint(0, len(word) - 2)\n",
        "            return word[:pos] + word[pos+1] + word[pos] + word[pos+2:]\n",
        "        \n",
        "        return word\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        words = re.findall(r'\\w+|\\W+', sentence)\n",
        "        misspelled_words = [introduce_error(w) if re.match(r'\\w+', w) else w for w in words]\n",
        "        misspelled_sentence = ''.join(misspelled_words)\n",
        "        \n",
        "        test_pairs.append((misspelled_sentence, sentence))\n",
        "    \n",
        "    return test_pairs\n",
        "\n",
        "def evaluate_correctors(test_set):\n",
        "    corpus_url = \"https://norvig.com/big.txt\"\n",
        "    \n",
        "    print(\"Initializing context-sensitive corrector...\")\n",
        "    context_corrector = ContextSpellingCorrector(corpus_url)\n",
        "    \n",
        "    print(\"Initializing Norvig corrector...\")\n",
        "    norvig_corrector = NorvigSpellChecker(corpus_url)\n",
        "    \n",
        "    print(\"Evaluating context-sensitive corrector...\")\n",
        "    context_results = {'corrected_words': 0, 'total_words': 0, 'time_taken': 0}\n",
        "    \n",
        "    start_time = time.time()\n",
        "    for misspelled, correct in test_set:\n",
        "        corrected = context_corrector.correct_text(misspelled)\n",
        "        \n",
        "        misspelled_words = re.findall(r'\\w+', misspelled.lower())\n",
        "        correct_words = re.findall(r'\\w+', correct.lower())\n",
        "        corrected_words = re.findall(r'\\w+', corrected.lower())\n",
        "        \n",
        "        for i, (m, c) in enumerate(zip(misspelled_words, correct_words)):\n",
        "            if i < len(corrected_words):\n",
        "                context_results['total_words'] += 1\n",
        "                if m != c and corrected_words[i] == c:\n",
        "                    context_results['corrected_words'] += 1\n",
        "    \n",
        "    context_results['time_taken'] = time.time() - start_time\n",
        "    context_results['accuracy'] = context_results['corrected_words'] / context_results['total_words'] if context_results['total_words'] > 0 else 0\n",
        "    \n",
        "    print(\"Evaluating Norvig corrector...\")\n",
        "    norvig_results = {'corrected_words': 0, 'total_words': 0, 'time_taken': 0}\n",
        "    \n",
        "    start_time = time.time()\n",
        "    for misspelled, correct in test_set:\n",
        "        corrected = norvig_corrector.correct_text(misspelled)\n",
        "        \n",
        "        misspelled_words = re.findall(r'\\w+', misspelled.lower())\n",
        "        correct_words = re.findall(r'\\w+', correct.lower())\n",
        "        corrected_words = re.findall(r'\\w+', corrected.lower())\n",
        "        \n",
        "        for i, (m, c) in enumerate(zip(misspelled_words, correct_words)):\n",
        "            if i < len(corrected_words):\n",
        "                norvig_results['total_words'] += 1\n",
        "                if m != c and corrected_words[i] == c:\n",
        "                    norvig_results['corrected_words'] += 1\n",
        "    \n",
        "    norvig_results['time_taken'] = time.time() - start_time\n",
        "    norvig_results['accuracy'] = norvig_results['corrected_words'] / norvig_results['total_words'] if norvig_results['total_words'] > 0 else 0\n",
        "    \n",
        "    return {'context': context_results, 'norvig': norvig_results}\n",
        "\n",
        "def main_evaluation():\n",
        "    corpus_url = \"https://norvig.com/big.txt\"\n",
        "    \n",
        "    if not os.path.exists(\"corpus.txt\"):\n",
        "        print(f\"Downloading corpus from {corpus_url}...\")\n",
        "        urllib.request.urlretrieve(corpus_url, \"corpus.txt\")\n",
        "    \n",
        "    with open(\"corpus.txt\", 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        corpus_text = f.read()\n",
        "    \n",
        "    noise_levels = [0.1, 0.2, 0.3]\n",
        "    results = {}\n",
        "    \n",
        "    for noise in noise_levels:\n",
        "        print(f\"\\nGenerating test set with noise probability {noise}...\")\n",
        "        test_set = generate_test_set(corpus_text, noise_probability=noise, test_size=50)\n",
        "        \n",
        "        print(f\"Evaluating correctors on test set with noise probability {noise}...\")\n",
        "        eval_results = evaluate_correctors(test_set)\n",
        "        results[noise] = eval_results\n",
        "        \n",
        "        # Print results\n",
        "        print(f\"\\nResults for noise probability {noise}:\")\n",
        "        print(f\"Context-sensitive corrector accuracy: {eval_results['context']['accuracy']:.4f}\")\n",
        "        print(f\"Norvig corrector accuracy: {eval_results['norvig']['accuracy']:.4f}\")\n",
        "        print(f\"Context-sensitive time: {eval_results['context']['time_taken']:.2f} seconds\")\n",
        "        print(f\"Norvig time: {eval_results['norvig']['time_taken']:.2f} seconds\")\n",
        "    \n",
        "    # Summary\n",
        "    print(\"\\nSummary:\")\n",
        "    print(\"Noise\\tContext\\tNorvig\\tImprovement\")\n",
        "    for noise in noise_levels:\n",
        "        context_acc = results[noise]['context']['accuracy']\n",
        "        norvig_acc = results[noise]['norvig']['accuracy']\n",
        "        improvement = (context_acc - norvig_acc) / norvig_acc * 100 if norvig_acc > 0 else float('inf')\n",
        "        print(f\"{noise:.1f}\\t{context_acc:.4f}\\t{norvig_acc:.4f}\\t{improvement:.2f}%\")\n",
        "\n",
        "\n",
        "corrector = ContextSpellingCorrector(\"https://norvig.com/big.txt\")\n",
        "\n",
        "test_sentences = [\n",
        "    \"I dking sport every day\",\n",
        "    \"The dking species is becoming rare\",\n",
        "    \"She has a bewtiful voice\",\n",
        "    \"The waether is nice today\",\n",
        "    \"I cant beleive I won the competiton\"\n",
        "]\n",
        "\n",
        "print(\"\\nTesting on example sentences:\")\n",
        "for sentence in test_sentences:\n",
        "    corrected = corrector.correct_text(sentence)\n",
        "    print(f\"Original: {sentence}\")\n",
        "    print(f\"Corrected: {corrected}\")\n",
        "    print()\n",
        "\n",
        "main_evaluation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Useful resources (also included in the archive in moodle):\n",
        "\n",
        "1. [Possible dataset with N-grams](https://www.ngrams.info/download_coca.asp)\n",
        "2. [Damerau–Levenshtein distance](https://en.wikipedia.org/wiki/Damerau–Levenshtein_distance#:~:text=Informally%2C%20the%20Damerau–Levenshtein%20distance,one%20word%20into%20the%20other.)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pmldl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
